"""
Main entry point â€” Cháº¡y SRE Agent workflow & stream events tá»›i Dashboard.
"""
import os
import json
import time
import requests
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI

from graph import build_sre_graph

load_dotenv()

DASHBOARD_URL = os.getenv("DASHBOARD_URL", "http://localhost:8888")


def emit_to_dashboard(event: dict):
    """Gá»­i event tá»›i dashboard qua REST API."""
    try:
        requests.post(
            f"{DASHBOARD_URL}/api/agent-log",
            json=event,
            timeout=2,
        )
    except Exception:
        pass  # Dashboard cÃ³ thá»ƒ chÆ°a cháº¡y


def run_sre_workflow():
    """Cháº¡y toÃ n bá»™ SRE workflow."""
    print("=" * 60)
    print("ğŸ¤– SRE Multi-Agent System â€” STRATUS Demo")
    print("=" * 60)

    # Init LLM
    llm = ChatOpenAI(
        model=os.getenv("OPENAI_MODEL", "gpt-4o-mini"),
        temperature=0,
    )

    # Build graph
    graph = build_sre_graph(llm)

    # Initial state
    initial_state = {
        "symptoms": [],
        "triage_summary": "",
        "overall_status": "",
        "services_health": [],
        "container_logs": "",
        "metrics_data": "",
        "tracing_data": "",
        "pre_fix_health": "",
        "mitigation_plan": None,
        "mitigation_result": None,
        "actions_taken": [],
        "attempt_count": 0,
        "max_retries": 3,
        "current_phase": "",
        "workflow_events": [],
    }

    emit_to_dashboard({
        "agent": "System",
        "message": "ğŸš€ SRE Agent Workflow báº¯t Ä‘áº§u...",
        "level": "info",
    })

    # Stream graph execution
    prev_events_count = 0
    for step in graph.stream(initial_state, stream_mode="updates"):
        for node_name, node_output in step.items():
            # Emit workflow events má»›i tá»›i dashboard
            events = node_output.get("workflow_events", [])
            new_events = events[prev_events_count:] if isinstance(events, list) else []
            for ev in new_events:
                emit_to_dashboard({
                    "agent": ev.get("agent", node_name),
                    "message": ev.get("action", ""),
                    "level": "info",
                    "phase": ev.get("phase", ""),
                    "type": ev.get("type", "action"),
                })
                time.sleep(0.5)  # Delay cho visual effect
            prev_events_count = len(events) if isinstance(events, list) else 0

            print(f"\nğŸ“ Node '{node_name}' completed.")

    emit_to_dashboard({
        "agent": "System",
        "message": "âœ… SRE Agent Workflow hoÃ n táº¥t!",
        "level": "info",
    })

    print("\n" + "=" * 60)
    print("âœ… Workflow hoÃ n táº¥t!")
    print("=" * 60)


if __name__ == "__main__":
    run_sre_workflow()


# """
# SRE Agent Demo â€” Entry point.
# Há»‡ thá»‘ng Multi-Agent tá»± Ä‘á»™ng phÃ¡t hiá»‡n vÃ  sá»­a lá»—i microservices.
# Láº¥y cáº£m há»©ng tá»« paper: STRATUS - A Multi-agent System for Autonomous Reliability Engineering.
# """
# import os
# import sys
# import argparse
# from dotenv import load_dotenv

# load_dotenv()

# # ThÃªm project root vÃ o path
# sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

# from langchain_openai import ChatOpenAI
# from graph import build_sre_graph


# def print_banner():
#     print("""
# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘          ğŸ¤– SRE Multi-Agent System (STRATUS-inspired)        â•‘
# â•‘          Autonomous Reliability Engineering Demo             â•‘
# â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
# â•‘  Agents:                                                     â•‘
# â•‘    ğŸ” Triage Agent    â€” Hybrid detection (Metrics + LLM)     â•‘
# â•‘    ğŸ“‹ Planner Agent   â€” Root cause analysis & planning       â•‘
# â•‘    ğŸ”§ Mitigation Agent â€” Execute fix                         â•‘
# â•‘    âª Undo Agent      â€” TNR (Transactional No-Regression)    â•‘
# â•‘                                                              â•‘
# â•‘  Services:                                                   â•‘
# â•‘    ğŸ“¦ Order Service   â€” Quáº£n lÃ½ Ä‘Æ¡n hÃ ng (orchestrator)      â•‘
# â•‘    ğŸ·ï¸  Product Service â€” Quáº£n lÃ½ sáº£n pháº©m & tá»“n kho          â•‘
# â•‘    ğŸ’³ Payment Service â€” Xá»­ lÃ½ thanh toÃ¡n                     â•‘
# â•‘    ğŸšª API Gateway     â€” Nginx reverse proxy                  â•‘
# â•‘                                                              â•‘
# â•‘  Observability:                                              â•‘
# â•‘    ğŸ“Š Prometheus (9090) | ğŸ” Jaeger (16686) | ğŸ“ˆ cAdvisor    â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#     """)


# def main():
#     parser = argparse.ArgumentParser(description="SRE Multi-Agent Demo")
#     parser.add_argument(
#         "--model", default="gpt-4o-mini", help="OpenAI model name (default: gpt-4o-mini)"
#     )
#     parser.add_argument(
#         "--max-retries", type=int, default=3, help="Max retry attempts for TNR (default: 3)"
#     )
#     args = parser.parse_args()

#     print_banner()

#     # Init LLM
#     api_key = os.getenv("OPENAI_API_KEY")
#     if not api_key:
#         print("âŒ OPENAI_API_KEY not found in .env!")
#         sys.exit(1)

#     llm = ChatOpenAI(model_name=args.model, temperature=0)
#     print(f"ğŸ¤– LLM: {args.model}")
#     print(f"ğŸ”„ Max retries (TNR): {args.max_retries}\n")

#     # Build graph
#     app = build_sre_graph(llm)

#     # Initial state
#     initial_state = {
#         "services_health": [],
#         "container_logs": "",
#         "metrics_data": "",
#         "tracing_data": "",
#         "symptoms": [],
#         "triage_summary": "",
#         "mitigation_plan": None,
#         "mitigation_result": None,
#         "attempt_count": 0,
#         "max_retries": args.max_retries,
#         "pre_fix_health": "",
#         "overall_status": "unknown",
#         "resolution": "",
#         "actions_taken": [],
#     }

#     print("=" * 60)
#     print("ğŸš€ Báº¯t Ä‘áº§u SRE Workflow...")
#     print("=" * 60)

#     try:
#         final_state = None
#         for output in app.stream(initial_state):
#             for key, value in output.items():
#                 if isinstance(value, dict):
#                     final_state = {**initial_state, **(final_state or {}), **value}

#         print("\n" + "=" * 60)
#         print("ğŸ“Š Káº¾T QUáº¢ WORKFLOW")
#         print("=" * 60)

#         if final_state:
#             status = final_state.get("overall_status", "unknown")
#             emoji = "âœ…" if status == "healthy" else "âš ï¸" if status == "degraded" else "âŒ"
#             print(f"   {emoji} Overall Status: {status.upper()}")
#             print(f"   ğŸ“ Triage Summary: {final_state.get('triage_summary', 'N/A')}")
#             print(f"   ğŸ”„ Attempts: {final_state.get('attempt_count', 0)}")

#             actions = final_state.get("actions_taken", [])
#             if actions:
#                 print(f"   ğŸ“‹ Actions taken:")
#                 for a in actions:
#                     print(f"      â€¢ {a}")

#             plan = final_state.get("mitigation_plan")
#             if plan:
#                 print(f"   ğŸ¯ Root Cause: {plan.root_cause if hasattr(plan, 'root_cause') else plan.get('root_cause', 'N/A')}")

#     except KeyboardInterrupt:
#         print("\n\nâš ï¸ Workflow interrupted by user.")
#     except Exception as e:
#         print(f"\nâŒ Error during workflow: {str(e)}")
#         import traceback
#         traceback.print_exc()

#     print("\n" + "=" * 60)
#     print("ğŸ Workflow execution completed.")
#     print("=" * 60)


# if __name__ == "__main__":
#     main()
